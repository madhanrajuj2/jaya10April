1. user defined functions (udf in spark) - 
https://www.youtube.com/watch?v=pZQsDloGB4w
2. data frame
3. difference's b/w data frame and RDD
4. Yarn architecture
5. what is driver with respect to job exection(speak in terms of spark)
6. yarn mode and local mode
7. MRv1 and Yarn working explanation
8. why we need Hbase when we have Hive
9. Avro file format advantages
10. have you used compression


11. different ways dataframe can be created
- It can be created using different data formats. For example, loading the data from JSON, CSV.
- Loading data from Existing RDD.
- Programmatically specifying schema

Good link to understand the concepts:
------------------------------------------
https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/


12. difference between RDD, DataFrame, DataSet

13. Asked about partitions - (sqlContext.setConf("spark.sql.suffle.patitions","10"))
  by default it will run 200 partitions

14. Hive merge command

15. what are different transformations you have used.
16. different stages once the file is read in Spark.
17. what is Trace in Scala
18. functional programing